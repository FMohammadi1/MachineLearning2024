# -*- coding: utf-8 -*-
"""Mini Project2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16CTi3LkbMtEQcNVT_sE5UqxUGqpI5pX9

پرسش اول

Q3

با استفاده از Class
"""

import numpy as np
import matplotlib.pyplot as plt

# تعریف نورون McCulloch-Pitts
class McCulloch_Pitts_neuron():
    def __init__(self, weights, threshold):
        self.weights = weights    # تعریف وزن‌ها
        self.threshold = threshold    # تعریف آستانه

    def model(self, x):
        # مدل با آستانه
        if np.dot(self.weights, x) >= self.threshold:
            return 1
        else:
            return 0

# تابع محاسبه مساحت مثلث
def area(x1, y1, x2, y2, x3, y3):
    return 0.5 * abs((x2 - x1) * (y3 - y1) - (x3 - x1) * (y2 - y1))

# تابع تعیین نقاط داخل مثلث
def is_inside_triangle(P, A, B, C):
    total_area = area(A[0], A[1], B[0], B[1], C[0], C[1])
    area1 = area(P[0], P[1], B[0], B[1], C[0], C[1])
    area2 = area(A[0], A[1], P[0], P[1], C[0], C[1])
    area3 = area(A[0], A[1], B[0], B[1], P[0], P[1])
    return np.isclose(total_area, area1 + area2 + area3)

# تابع مدل برای تعیین نقاط داخل مثلث
def Area(x, y):
    # تعریف نقاط مثلث
    A = np.array([2, 2])
    B = np.array([3, 0])
    C = np.array([1, 0])

    # تعریف نورون‌ها برای هر خط
    neur1 = McCulloch_Pitts_neuron([0, 1], 0)     # خط AB: y > 0
    neur2 = McCulloch_Pitts_neuron([-2, 1], 6)    # خط BC: y > -2x + 6
    neur3 = McCulloch_Pitts_neuron([2, -1], 2)    # خط CA: y < 2x - 2

    # خروجی نورون‌ها
    z1 = neur1.model(np.array([x, y]))   # خط AB
    z2 = neur2.model(np.array([x, y]))   # خط BC
    z3 = neur3.model(np.array([x, y]))   # خط CA

    # نورون نهایی برای تعیین وضعیت داخل یا خارج مثلث
    if is_inside_triangle(np.array([x, y]), A, B, C):
        return [1]  # داخل مثلث
    else:
        return [0]  # خارج مثلث

# تولید نقاط تصادفی
num_points = 2000
x_values = np.random.uniform(0, 4, num_points)
y_values = np.random.uniform(-1, 3, num_points)

# لیست‌ها برای ذخیره نقاط داخل و خارج مثلث
red_points = []
green_points = []

# ارزیابی نقاط با استفاده از تابع Area
for i in range(num_points):
    z5_value = Area(x_values[i], y_values[i])
    if z5_value == [0]:  # اگر نقطه خارج مثلث باشد
        red_points.append((x_values[i], y_values[i]))
    else:  # اگر نقطه داخل مثلث باشد
        green_points.append((x_values[i], y_values[i]))

# بررسی این که آیا نقاط داخل مثلث موجود است یا خیر
if len(green_points) == 0:
    print("هیچ نقطه‌ای داخل مثلث قرار ندارد!")

# جداسازی مقادیر x و y برای نقاط قرمز و سبز
if len(red_points) > 0:
    red_x, red_y = zip(*red_points)
else:
    red_x, red_y = [], []

if len(green_points) > 0:
    green_x, green_y = zip(*green_points)
else:
    green_x, green_y = [], []

# رسم نمودار
plt.figure(figsize=(10, 5))

# نمودار اول: نقاط و مثلث
plt.subplot(1, 2, 1)
if red_x:
    plt.scatter(red_x, red_y, color='red', label='Outside Triangle')
if green_x:
    plt.scatter(green_x, green_y, color='green', label='Inside Triangle')
plt.xlabel('X values')
plt.ylabel('Y values')
plt.title('McCulloch-Pitts Neuron Outputs')
plt.legend()

# مختصات رئوس مثلث
A = [2, 2]
B = [3, 0]
C = [1, 0]

# رسم خطوط مرز مثلث
plt.plot([A[0], B[0]], [A[1], B[1]], 'b')
plt.plot([B[0], C[0]], [B[1], C[1]], 'b')
plt.plot([C[0], A[0]], [C[1], A[1]], 'b')

# پر کردن مثلث با رنگ آبی
plt.fill([A[0], B[0], C[0]], [A[1], B[1], C[1]], 'b', alpha=0.1)
plt.xlabel('X values')
plt.ylabel('Y values')
plt.title('Graph with Lines and Hatched Area (Triangle)')
plt.xlim(0, 4)
plt.ylim(-1, 3)
plt.grid(True)

# نمودار دوم: خطوط مرز و نواحی هاشورزده
plt.subplot(1, 2, 2)
# رسم خطوط مرز مثلث
plt.plot([A[0], B[0]], [A[1], B[1]], 'b')
plt.plot([B[0], C[0]], [B[1], C[1]], 'b')
plt.plot([C[0], A[0]], [C[1], A[1]], 'b')

# پر کردن مثلث با رنگ آبی
plt.fill([A[0], B[0], C[0]], [A[1], B[1], C[1]], 'b', alpha=0.1)
plt.xlabel('X values')
plt.ylabel('Y values')
plt.title('Graph with Lines and Hatched Area (Triangle)')
plt.xlim(0, 4)
plt.ylim(-1, 3)
plt.grid(True)

# نمایش نمودارها
plt.tight_layout()
plt.show()

"""بدون استفاده از کلاس"""

import numpy as np
import matplotlib.pyplot as plt

# تولید نقاط رندوم با محدوده x از 0 تا 4 و y از -1 تا 3
np.random.seed(42)
points = np.random.rand(2000, 2)
points[:, 0] *= 4  # محدوده x از 0 تا 4
points[:, 1] = points[:, 1] * 4 - 1  # محدوده y از -1 تا 3

# تعریف نقاط مثلث
A = np.array([2, 2])
B = np.array([3, 0])
C = np.array([1, 0])

# تابع محاسبه مساحت مثلث
def area(x1, y1, x2, y2, x3, y3):
    return 0.5 * abs((x2 - x1) * (y3 - y1) - (x3 - x1) * (y2 - y1))

# تابع تعیین نقاط داخل مثلث
def is_inside_triangle(P, A, B, C):
    total_area = area(A[0], A[1], B[0], B[1], C[0], C[1])
    area1 = area(P[0], P[1], B[0], B[1], C[0], C[1])
    area2 = area(A[0], A[1], P[0], P[1], C[0], C[1])
    area3 = area(A[0], A[1], B[0], B[1], P[0], P[1])
    return np.isclose(total_area, area1 + area2 + area3)

# دسته بندی نقاط
inside_points = []
outside_points = []

for point in points:
    if is_inside_triangle(point, A, B, C):
        inside_points.append(point)
    else:
        outside_points.append(point)

inside_points = np.array(inside_points)
outside_points = np.array(outside_points)

# رسم نمودار
plt.figure(figsize=(10, 5))

# نمودار نقاط و مثلث
plt.subplot(1, 2, 1)
plt.plot([A[0], B[0]], [A[1], B[1]], 'b')
plt.plot([B[0], C[0]], [B[1], C[1]], 'b')
plt.plot([C[0], A[0]], [C[1], A[1]], 'b')
plt.fill([A[0], B[0], C[0]], [A[1], B[1], C[1]], 'b', alpha=0.1)
plt.scatter(outside_points[:, 0], outside_points[:, 1], color='r', label='Outside Triangle')
plt.scatter(inside_points[:, 0], inside_points[:, 1], color='g', label='Inside Triangle')
plt.legend()
plt.title("McCulloch-Pitts Neuron Outputs")
plt.xlabel("X-axis")
plt.ylabel("Y-axis")
plt.xlim(0, 4)  # محدوده x از 0 تا 4
plt.ylim(-1, 3)  # محدوده y از -1 تا 3

# نمودار هاشورزده
plt.subplot(1, 2, 2)
plt.plot([A[0], B[0]], [A[1], B[1]], 'b')
plt.plot([B[0], C[0]], [B[1], C[1]], 'b')
plt.plot([C[0], A[0]], [C[1], A[1]], 'b')
plt.fill([A[0], B[0], C[0]], [A[1], B[1], C[1]], 'b', alpha=0.1)
plt.title("Graph with Lines and Hatched Area (Triangle)")
plt.xlabel("X-axis")
plt.ylabel("Y-axis")
plt.xlim(0, 4)  # محدوده x از 0 تا 4
plt.ylim(-1, 3)  # محدوده y از -1 تا 3

plt.tight_layout()
plt.show()

"""اضافه شدن تابع های فعال سازی"""

import numpy as np
import matplotlib.pyplot as plt

# تعریف نورون McCulloch-Pitts با تابع فعال‌ساز
class McCulloch_Pitts_neuron():
    def __init__(self, weights, threshold, activation_function=None):
        self.weights = weights
        self.threshold = threshold
        self.activation_function = activation_function

    def model(self, x):
        output = np.dot(self.weights, x)

        if self.activation_function:
            output = self.activation_function(output)

        if output >= self.threshold:
            return 1
        else:
            return 0

# تابع محاسبه مساحت مثلث
def area(x1, y1, x2, y2, x3, y3):
    return 0.5 * abs((x2 - x1) * (y3 - y1) - (x3 - x1) * (y2 - y1))

# تابع تعیین نقاط داخل مثلث
def is_inside_triangle(P, A, B, C):
    total_area = area(A[0], A[1], B[0], B[1], C[0], C[1])
    area1 = area(P[0], P[1], B[0], B[1], C[0], C[1])
    area2 = area(A[0], A[1], P[0], P[1], C[0], C[1])
    area3 = area(A[0], A[1], B[0], B[1], P[0], P[1])
    return np.isclose(total_area, area1 + area2 + area3)

# تابع مدل برای تعیین نقاط داخل مثلث
def Area(x, y, activation_function):
    A = np.array([2, 2])
    B = np.array([3, 0])
    C = np.array([1, 0])

    neur1 = McCulloch_Pitts_neuron([0, 1], 0, activation_function)
    neur2 = McCulloch_Pitts_neuron([-2, 1], 6, activation_function)
    neur3 = McCulloch_Pitts_neuron([2, -1], 2, activation_function)

    z1 = neur1.model(np.array([x, y]))
    z2 = neur2.model(np.array([x, y]))
    z3 = neur3.model(np.array([x, y]))

    if is_inside_triangle(np.array([x, y]), A, B, C):
        return [1]
    else:
        return [0]

# توابع فعال‌ساز
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def relu(x):
    return np.maximum(0, x)

# تولید نقاط تصادفی
num_points = 2000
x_values = np.random.uniform(0, 4, num_points)
y_values = np.random.uniform(-1, 3, num_points)

# لیست‌ها برای ذخیره نقاط داخل و خارج مثلث
red_points = []
green_points = []

activation_functions = [sigmoid, relu]

for activation_function in activation_functions:
    red_points_func = []
    green_points_func = []

    for i in range(num_points):
        z5_value = Area(x_values[i], y_values[i], activation_function)
        if z5_value == [0]:
            red_points_func.append((x_values[i], y_values[i]))
        else:
            green_points_func.append((x_values[i], y_values[i]))

    if len(green_points_func) == 0:
        print(f"هیچ نقطه‌ای داخل مثلث قرار ندارد برای تابع فعال‌ساز {activation_function.__name__}!")

    if len(red_points_func) > 0:
        red_x_func, red_y_func = zip(*red_points_func)
    else:
        red_x_func, red_y_func = [], []

    if len(green_points_func) > 0:
        green_x_func, green_y_func = zip(*green_points_func)
    else:
        green_x_func, green_y_func = [], []

    # رسم نمودار
    plt.figure(figsize=(10, 5))

    # نمودار اول: نقاط و مثلث
    plt.subplot(1, 2, 1)
    if len(red_x_func) > 0:
        plt.scatter(red_x_func, red_y_func, color='red', label='Outside Triangle')
    if len(green_x_func) > 0:
        plt.scatter(green_x_func, green_y_func, color='green', label='Inside Triangle')
    plt.xlabel('X values')
    plt.ylabel('Y values')
    plt.title(f'McCulloch-Pitts Neuron Outputs with {activation_function.__name__} Activation Function')
    plt.legend()

    # رسم خطوط مرز مثلث
    A = [2, 2]
    B = [3, 0]
    C = [1, 0]
    plt.plot([A[0], B[0]], [A[1], B[1]], 'b')
    plt.plot([B[0], C[0]], [B[1], C[1]], 'b')
    plt.plot([C[0], A[0]], [C[1], A[1]], 'b')
    plt.fill([A[0], B[0], C[0]], [A[1], B[1], C[1]], 'b', alpha=0.1)
    plt.xlabel('X values')
    plt.ylabel('Y values')
    plt.xlim(0, 4)
    plt.ylim(-1, 3)
    plt.grid(True)

    # نمایش نمودارها
    plt.tight_layout()
    plt.show()



# محاسبه نرخ خطا
def calculate_error_rate(points, A, B, C, threshold=0.1):
    total_near_boundary = 0
    total_errors = 0

    for x, y in points:
        d1 = point_line_distance(x, y, A[0], A[1], B[0], B[1])
        d2 = point_line_distance(x, y, B[0], B[1], C[0], C[1])
        d3 = point_line_distance(x, y, C[0], C[1], A[0], A[1])

        if min(d1, d2, d3) < threshold:
            total_near_boundary += 1
            inside_triangle = is_inside_triangle([x, y], A, B, C)
            predicted = Area(x, y, sigmoid)
            if inside_triangle != predicted:
                total_errors += 1

    error_rate = total_errors / total_near_boundary if total_near_boundary > 0 else 0
    return error_rate

# محاسبه نرخ خطا
error_rate = calculate_error_rate(zip(x_values, y_values), A, B, C)

# نمایش نرخ خطا
print(f"Error Rate: {error_rate:.2%}")

"""پرسش دوم

Q1
"""

import pandas as pd
#!/bin/bash
!kaggle datasets download aslkuscu/telecust1000t
!unzip telecust1000t.zip

# بارگذاری داده‌ها

df = pd.read_csv('/content/teleCust1000t.csv')

"""Q2"""

import seaborn as sns
import matplotlib.pyplot as plt

# محاسبه ماتریس همبستگی
corr_matrix = df.corr()

# ایجاد Heatmap
plt.figure(figsize=(10, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5, annot_kws={"size": 8}, fmt='.3f')
plt.title("Correlation Matrix")
plt.show()

# شناسایی ویژگی‌ها با بیشترین همبستگی با 'telecust1000t'
target_corr = corr_matrix['custcat'].sort_values(ascending=False)
top_features = target_corr.head(2).index

# رسم هیستوگرام
plt.figure(figsize=(10, 6))
for feature in top_features:
    plt.subplot(1, 2, list(top_features).index(feature) + 1)
    sns.histplot(df[feature], kde=True)
    plt.title(f'Histogram of {feature}')
plt.tight_layout()
plt.show()

"""Q3"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Read the CSV file
data = pd.read_csv('/content/teleCust1000t.csv')

# Separate the label and data
label = df['custcat']
data = df.drop('custcat', axis=1)

# Remove the header from the variables
data = data.values
label = label.values

# Split the data into train/validation/test sets
train_data, test_data, train_label, test_label = train_test_split(data, label, test_size=0.15, random_state=42)
train_data, val_data, train_label, val_label = train_test_split(train_data, train_label, test_size=0.15, random_state=42)

# Scale the data using MinMaxScaler
scaler = MinMaxScaler()
train_data = scaler.fit_transform(train_data)
val_data = scaler.transform(val_data)
test_data = scaler.transform(test_data)

# Print shape of each set
print(f"train_data shape: {train_data.shape}")
print(f"val_data shape: {val_data.shape}")
print(f"test_data shape: {test_data.shape}")
print(f"train_label shape: {train_label.shape}")
print(f"val_label shape: {val_label.shape}")
print(f"test_label shape: {test_label.shape}")

# Save the train/validation/test data/label to new files
pd.DataFrame(train_data).to_csv('/content/finaltrain_data.csv', index=False, header=False)
pd.DataFrame(val_data).to_csv('/content/finalval_data.csv', index=False, header=False)
pd.DataFrame(test_data).to_csv('/content/finaltest_data.csv', index=False, header=False)
pd.DataFrame(train_label).to_csv('/content/finaltrain_label.csv', index=False, header=False)
pd.DataFrame(val_label).to_csv('/content/finalval_label.csv', index=False, header=False)
pd.DataFrame(test_label).to_csv('/content/finaltest_label.csv', index=False, header=False)

"""Q4"""

import torch
import torch.nn as nn
import torch.optim as optim

# تعریف مدل MLP
class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim1)
        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)
        self.fc3 = nn.Linear(hidden_dim2, output_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

class MLP_with_BN(nn.Module):
    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):
        super(MLP_with_BN, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim1)
        self.bn1 = nn.BatchNorm1d(hidden_dim1)
        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)
        self.bn2 = nn.BatchNorm1d(hidden_dim2)
        self.fc3 = nn.Linear(hidden_dim2, output_dim)

    def forward(self, x):
        x = torch.relu(self.bn1(self.fc1(x)))
        x = torch.relu(self.bn2(self.fc2(x)))
        x = self.fc3(x)
        return x

class MLP_with_Dropout(nn.Module):
    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):
        super(MLP_with_Dropout, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim1)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)
        self.dropout2 = nn.Dropout(0.5)
        self.fc3 = nn.Linear(hidden_dim2, output_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = self.fc3(x)
        return x

!pip install --upgrade scikit-learn
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt

device = torch.device('cpu')

# Define the training function
def train(model, optimizer, criterion, train_data, train_label, val_data, val_label, num_epochs=1000):
    train_loss_list = []
    val_loss_list = []
    r2score_list = []
    train_r2score_list = []
    best_model = None
    best_r2score = -1

    for epoch in range(num_epochs):
        # Training
        model.train()
        optimizer.zero_grad()
        train_output = model(train_data)
        train_loss = criterion(train_output.squeeze(), train_label)
        train_loss.backward()
        optimizer.step()
        train_loss_list.append(train_loss.item())
        train_r2score = r2_score(train_label.cpu().numpy(), train_output.squeeze().cpu().detach().numpy())
        train_r2score_list.append(train_r2score)

        # Validation
        model.eval()
        with torch.no_grad():
            val_output = model(val_data)
            val_loss = criterion(val_output.squeeze(), val_label)
            val_loss_list.append(val_loss.item())
            r2score = r2_score(val_label.cpu().numpy(), val_output.squeeze().cpu().detach().numpy())
            r2score_list.append(r2score)
            if r2score > best_r2score:
                best_r2score = r2score
                best_model = model.state_dict()

        # Print loss and R2 score every 100 epochs
        if (epoch+1) % 5 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}, Train R2 score: {train_r2score:.4f}, Val R2 score: {r2score:.4f}")

    # Plot loss curve and save as pdf
    plt.plot(train_loss_list, label='Train Loss')
    plt.plot(val_loss_list, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title(type(model).__name__)
    plt.savefig(f"{type(model).__name__}_loss.pdf")
    plt.show()

    # Plot R2 score curve and save as pdf
    plt.plot(train_r2score_list, label='Train R2 score')
    plt.plot(r2score_list, label='Val R2 score')
    plt.xlabel('Epoch')
    plt.ylabel('R2 score')
    plt.legend()
    plt.title(type(model).__name__)
    plt.savefig(f"{type(model).__name__}_r2score.pdf")
    plt.show()

    # Return the best model based on the validation R2 score
    return best_model

# Define the data and labels
train_data = torch.Tensor(train_data)
train_label = torch.Tensor(train_label)
val_data = torch.Tensor(val_data)
val_label = torch.Tensor(val_label)
test_data = torch.Tensor(test_data)
test_label = torch.Tensor(test_label)

# Define the hyperparameters
input_dim = 11
output_dim = 1
hidden_dim1 = 16
hidden_dim2 = 64
hidden_dim3 = 128
learning_rate = 0.001
num_epochs = 120

# Train the models
model1 = MLP(input_dim, hidden_dim1, hidden_dim2, output_dim)
optimizer1 = optim.Adam(model1.parameters(), lr=learning_rate)
criterion1 = nn.MSELoss()
best_model1 = train(model1, optimizer1, criterion1, train_data, train_label, val_data, val_label, num_epochs=num_epochs)

model2 = MLP_with_BN(input_dim, hidden_dim1, hidden_dim2, output_dim)
optimizer2 = optim.Adam(model2.parameters(), lr=learning_rate)
criterion2 = nn.MSELoss()
best_model2 = train(model2, optimizer2, criterion2, train_data, train_label, val_data, val_label, num_epochs=num_epochs)

model3 = MLP_with_Dropout(input_dim, hidden_dim1, hidden_dim2, output_dim)
optimizer3 = optim.Adam(model3.parameters(), lr=learning_rate)
criterion3 = nn.MSELoss()
best_model3 = train(model3, optimizer3, criterion3, train_data, train_label, val_data, val_label, num_epochs=num_epochs)

# Evaluate the models on the test set
model1.load_state_dict(best_model1)
model1.eval()
with torch.no_grad():
    test_output1 = model1(test_data)
    test_loss1 = criterion1(test_output1.squeeze(), test_label)
    test_r2score1 = r2_score(test_label.cpu().numpy().astype(float), test_output1.squeeze().cpu().detach().numpy().astype(float))

model2.load_state_dict(best_model2)
model2.eval()
with torch.no_grad():
    test_output2 = model2(test_data)
    test_loss2 = criterion2(test_output2.squeeze(), test_label)
    test_r2score2 = r2_score(test_label.cpu().numpy(), test_output2.cpu().detach().numpy())

model3.load_state_dict(best_model3)
model3.eval()
with torch.no_grad():
    test_output3 = model3(test_data)
    test_loss3 = criterion3(test_output3.squeeze(), test_label)
    test_r2score3 = r2_score(test_label.cpu().numpy(), test_output3.squeeze().cpu().detach().numpy())

# Print the test set results
print("MLP1 Test Set Results:")
print(f"Loss: {test_loss1:.4f}, R2 Score: {test_r2score1:.4f}")

print("MLP2 Test Set Results:")
print(f"Loss: {test_loss2:.4f}, R2 Score: {test_r2score2:.4f}")

print("MLP3 Test Set Results:")
print(f"Loss: {test_loss3:.4f}, R2 Score: {test_r2score3:.4f}")

"""Q5"""

import random

# انتخاب 10 نمونه به صورت تصادفی از داده‌های تست
indices = random.sample(range(len(test_data)), 10)
test_samples = test_data[indices]
true_labels = test_label[indices]

# پیش‌بینی خروجی‌ها از مدل‌ها
model1.eval()
with torch.no_grad():
    output1 = model1(test_samples)
    output1 = output1.squeeze()

model2.eval()
with torch.no_grad():
    output2 = model2(test_samples)
    output2 = output2.squeeze()

# چاپ خروجی‌های شبکه‌ها همراه با مقادیر واقعی
for i, idx in enumerate(indices):
    print(f"Sample {idx+1}: True Label: {true_labels[i].item()}, Model 1 Output: {output1[i].item()}, Model 2 Output: {output2[i].item()}")

"""Q6"""

# ترکیب مدل‌ها
def ensemble_predict(models, input_data):
    predictions = []
    for model in models:
        model.eval()
        with torch.no_grad():
            predictions.append(model(input_data).unsqueeze(0))  # ذخیره پیش‌بینی هر مدل
    return torch.mean(torch.cat(predictions, dim=0), dim=0)  # میانگین پیش‌بینی‌ها

# ارزیابی مدل ترکیبی
models = [model1, model2]  # مدل‌های انتخاب شده
combined_output = ensemble_predict(models, test_data)

# Specify the criterion to be used, for example criterion1
criterion = criterion1  # or criterion2, criterion3 depending on which loss function you want to use

# محاسبه loss و R2 score برای مدل ترکیبی
combined_loss = criterion(combined_output.squeeze(), test_label)
combined_r2score = r2_score(test_label.cpu().numpy(), combined_output.squeeze().cpu().detach().numpy())

print("Ensemble Model Test Set Results:")
print(f"Loss: {combined_loss:.4f}, R2 Score: {combined_r2score:.4f}")

"""پرسش سوم"""

!pip install --upgrade --no-cache-dir gdown
!gdown 1QTi7dJtNAfFR5mG0rd8K3ZGvEIfSn_DS
!unzip PersianData.zip

"""Q1"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile convertImage.py
# 
# from PIL import Image, ImageDraw
# import random
# 
# def convertImageToBinary(path):
#     """
#     Convert an image to a binary representation based on pixel intensity.
# 
#     Args:
#         path (str): The file path to the input image.
# 
#     Returns:
#         list: A binary representation of the image where white is represented by -1 and black is represented by 1.
#     """
#     # Open the image file.
#     image = Image.open(path)
# 
#     # Create a drawing tool for manipulating the image.
#     draw = ImageDraw.Draw(image)
# 
#     # Determine the image's width and height in pixels.
#     width = image.size[0]
#     height = image.size[1]
# 
#     # Load pixel values for the image.
#     pix = image.load()
# 
#     # Define a factor for intensity thresholding.
#     factor = 100
# 
#     # Initialize an empty list to store the binary representation.
#     binary_representation = []
# 
#     # Loop through all pixels in the image.
#     for i in range(width):
#         for j in range(height):
#             # Extract the Red, Green, and Blue (RGB) values of the pixel.
#             red = pix[i, j][0]
#             green = pix[i, j][1]
#             blue = pix[i, j][2]
# 
#             # Calculate the total intensity of the pixel.
#             total_intensity = red + green + blue
# 
#             # Determine whether the pixel should be white or black based on the intensity.
#             if total_intensity > (((255 + factor) // 2) * 3):
#                 red, green, blue = 255, 255, 255  # White pixel
#                 binary_representation.append(-1)
#             else:
#                 red, green, blue = 0, 0, 0  # Black pixel
#                 binary_representation.append(1)
# 
#             # Set the pixel color accordingly.
#             draw.point((i, j), (red, green, blue))
# 
#     # Clean up the drawing tool.
#     del draw
# 
#     # Return the binary representation of the image.
#     return binary_representation
# 
#

from PIL import Image, ImageDraw
import random
import matplotlib.pyplot as plt

def generateNoisyImages():
    # List of image file paths
    image_paths = [
        "/content/1.jpg",
        "/content/2.jpg",
        "/content/3.jpg",
        "/content/4.jpg",
        "/content/5.jpg"
    ]

    for i, image_path in enumerate(image_paths, start=1):
        noisy_image_path = f"/content/noisy{i}.jpg"
        getNoisyBinaryImage(image_path, noisy_image_path)
        print(f"Noisy image for {image_path} generated and saved as {noisy_image_path}")

def getNoisyBinaryImage(input_path, output_path):
    """
    Add noise to an image and save it as a new file.

    Args:
        input_path (str): The file path to the input image.
        output_path (str): The file path to save the noisy image.
    """
    # Open the input image.
    image = Image.open(input_path)

    # Create a drawing tool for manipulating the image.
    draw = ImageDraw.Draw(image)

    # Determine the image's width and height in pixels.
    width = image.size[0]
    height = image.size[1]

    # Load pixel values for the image.
    pix = image.load()

    # Define a factor for introducing noise.
    noise_factor = 100

    # Loop through all pixels in the image.
    for i in range(width):
        for j in range(height):
            # Generate a random noise value within the specified factor.
            rand = random.randint(-noise_factor, noise_factor)

            # Add the noise to the Red, Green, and Blue (RGB) values of the pixel.
            red = pix[i, j][0] + rand
            green = pix[i, j][1] + rand
            blue = pix[i, j][2] + rand

            # Ensure that RGB values stay within the valid range (0-255).
            if red < 0:
                red = 0
            if green < 0:
                green = 0
            if blue < 0:
                blue = 0
            if red > 255:
                red = 255
            if green > 255:
                green = 255
            if blue > 255:
                blue = 255

            # Set the pixel color accordingly.
            draw.point((i, j), (red, green, blue))

    # Save the noisy image as a file.
    image.save(output_path, "JPEG")

    # Show the noisy image
    plt.imshow(image)
    plt.axis('off')  # Hide axes
    plt.show()

    # Clean up the drawing tool.
    del draw

# Generate noisy images and save them
generateNoisyImages()

"""بهبود یافته"""

from PIL import Image, ImageDraw
import random
import matplotlib.pyplot as plt

def getNoisyBinaryImage(input_path, output_path, num_missing_points, conversion_percentage):
    """
    Add noise to an image, generate missing points, and save it as a new file.

    Args:
        input_path (str): The file path to the input image.
        output_path (str): The file path to save the noisy image.
        num_missing_points (int): The number of missing points to generate.
        conversion_percentage (float): The percentage of black pixels to convert to white.
    """
    # Open the input image.
    image = Image.open(input_path)

    # Create a drawing tool for manipulating the image.
    draw = ImageDraw.Draw(image)

    # Determine the image's width and height in pixels.
    width = image.size[0]
    height = image.size[1]

    # Load pixel values for the image.
    pix = image.load()

    # Define a factor for introducing noise.
    noise_factor = 5

    # Loop through all pixels in the image.
    for i in range(width):
        for j in range(height):
            # Generate a random noise value within the specified factor.
            rand = random.randint(-noise_factor, noise_factor)

            # Add the noise to the Red, Green, and Blue (RGB) values of the pixel.
            red = pix[i, j][0] + rand
            green = pix[i, j][1] + rand
            blue = pix[i, j][2] + rand

            # Ensure that RGB values stay within the valid range (0-255).
            if red < 0:
                red = 0
            if green < 0:
                green = 0
            if blue < 0:
                blue = 0
            if red > 255:
                red = 255
            if green > 255:
                green = 255
            if blue > 255:
                blue = 255

            # Convert some black pixels to white based on the conversion percentage.
            if (red, green, blue) == (0, 0, 0) and random.random() < conversion_percentage:
                red, green, blue = 255, 255, 255

            # Set the pixel color accordingly.
            draw.point((i, j), (red, green, blue))

    # Generate missing points in the image.
    for _ in range(num_missing_points):
        x = random.randint(0, width - 1)
        y = random.randint(0, height - 1)
        draw.point((x, y), (255, 255, 255))  # Set the missing point to white

    # Save the noisy image as a file.
    image.save(output_path, "JPEG")

    # Show the noisy image
    plt.imshow(image)
    plt.axis('off')  # Hide axes
    plt.show()

    # Clean up the drawing tool.
    del draw

from PIL import Image, ImageDraw
import random

def generateNoisyImages():
    # List of image file paths
    image_paths = [
        "/content/1.jpg",
        "/content/2.jpg",
        "/content/3.jpg",
        "/content/4.jpg",
        "/content/5.jpg"
    ]

    for i, image_path in enumerate(image_paths, start=1):
        noisy_image_path = f"/content/noisy{i}.jpg"
        # Specify the number of missing points and conversion percentage here
        getNoisyBinaryImage(image_path, noisy_image_path, num_missing_points=500, conversion_percentage=0.1)
        print(f"Noisy image for {image_path} generated and saved as {noisy_image_path}")

# Generate noisy images with missing points and black-to-white conversion
generateNoisyImages()

"""Q2"""

#333-533

from pylab import *
from math import sqrt
import matplotlib.pyplot as plt
import os

# Define the path to the input image
IMAGE_PATH = "/content/noisy3.jpg"

def show(matrix):
    """
    Display a matrix in a formatted manner.

    Args:
        matrix (list of lists): The matrix to be displayed.
    """
    for j in range(len(matrix)):
        for i in range(len(matrix[0])):
            print("{:3f}".format(matrix[j][i]), end=" ")
        print(sep="")

def change(vector, a, b):
    """
    Transform a vector into a matrix of specified dimensions.

    Args:
        vector (list): The vector to be transformed.
        a (int): The number of columns in the resulting matrix.
        b (int): The number of rows in the resulting matrix.

    Returns:
        list of lists: The transformed matrix.
    """
    matrix = [[0 for j in range(a)] for i in range(b)]
    k = 0
    j = 0
    while k < b:
        i = 0
        while i < a:
            matrix[k][i] = vector[j]
            j += 1
            i += 1
        k += 1
    return matrix

def product(matrix, vector, T):
    """
    Multiply a matrix by a vector.

    Args:
        matrix (list of lists): The matrix to be multiplied.
        vector (list): The vector to be multiplied.
        T (float): The threshold parameter for the activation function.

    Returns:
        list: The resulting vector after multiplication.
    """
    result_vector = []
    for i in range(len(matrix)):
        x = 0
        for j in range(len(vector)):
            x = x + matrix[i][j] * vector[j]
        result_vector.append((x + T))
    return result_vector

def action(vector, T, Emax):
    """
    Activation function to process a vector.

    Args:
        vector (list): The input vector to be processed.
        T (float): The threshold parameter for the activation function.
        Emax (float): The maximum allowable value for the difference in output vectors between consecutive iterations.

    Returns:
        list: The output vector after activation.
    """
    result_vector = []
    for value in vector:
        if value <= 0:
            result_vector.append(0)
        elif 0 < value <= T:
            result_vector.append(Emax * value)
        elif value > T:
            result_vector.append(T)
    return result_vector

def mysum(vector, j):
    """
    Calculate the sum of vector values excluding the element at index j.

    Args:
        vector (list): The input vector.
        j (int): The index of the element to be excluded from the sum.

    Returns:
        float: The sum of vector values with the element at index j excluded.
    """
    p = 0
    total_sum = 0
    while p < len(vector):
        if p != j:
            total_sum = total_sum + vector[p]
        p += 1
    return total_sum

def norm(vector, p):
    """
    Calculate the difference between two vectors and compute the norm of the resulting vector.

    Args:
        vector (list): The first vector.
        p (list): The second vector for subtraction.

    Returns:
        float: The Euclidean norm of the difference between the two vectors.
    """
    difference = []
    for i in range(len(vector)):
        difference.append(vector[i] - p[i])
    sum = 0
    for element in difference:
        sum += element * element
    return sqrt(sum)

# List of paths to example images
path = [
    '/content/1.jpg',
    '/content/2.jpg',
    '/content/3.jpg',
    '/content/4.jpg',
    '/content/5.jpg',
]

x = []  # Binary representations of example images
print(os.path.basename(IMAGE_PATH))

# Convert and store binary representations of example images
for i in path:
    x.append(convertImageToBinary(i))

y = convertImageToBinary(IMAGE_PATH)  # Binary representation of the input image
entr = y
k = len(x)  # Number of example images
a = 96  # Number of columns in the transformed matrix
b = 96  # Number of rows in the transformed matrix
entr = y
q = change(y, a, b)  # Transformation of input image into a matrix
plt.matshow(q)
plt.colorbar()

m = len(x[0])
w = [[(x[i][j]) / 2 for j in range(m)] for i in range(k)]  # Weight matrix
T = m / 2  # Activation function threshold parameter
e = round(1 / len(x), 1)
E = [[0 for j in range(k)] for i in range(k)]  # Synaptic connection matrix
Emax = 0.000001  # Maximum allowable difference norm between output vectors in consecutive iterations
U = 1 / Emax

# Set values for the synaptic connection matrix
for i in range(k):
    for j in range(k):
        if j == i:
            E[i][j] = 1.0
        else:
            E[i][j] = -e

s = [product(w, y, T)]  # Initial output vector
p = action(s[0], U, Emax)
y = [p]
i = 0
j = []
p = [0 for j in range(len(s[0]))]

# Iterate until the difference norm is less than Emax
while norm(y[i], p) >= Emax:
    s.append([0 for j in range(len(s[0]))])
    for j in range(len(s[0])):
        s[i + 1][j] = y[i][j] - e * mysum(y[i], j)
    y.append((action(s[i + 1], U, Emax)))
    i += 1
    p = y[i - 1]

print('Output Vectors Table:')
show(y)
print('Last Output Vector:', *y[len(y) - 1])

# Determine the class with the highest output value
result_index = y[len(y) - 1].index(max(y[len(y) - 1])) + 1

if max(y[len(y) - 1]) == 0:
    print("The Hamming network cannot make a preference between classes.")
    print("In the case of a small number of input characteristics, the network may not be able to classify the image.")
    plt.show()
    exit()
else:
    q = change(x[result_index - 1], a, b)
    print('The highest positive output value is associated with class', result_index)
    plt.matshow(q)
    plt.colorbar()
    plt.show()

"""Q3"""

from PIL import Image, ImageDraw
import random

def getNoisyBinaryImage(input_path, output_path, num_missing_points, conversion_percentage):
    """
    Add noise to an image, generate missing points, and save it as a new file.

    Args:
        input_path (str): The file path to the input image.
        output_path (str): The file path to save the noisy image.
        num_missing_points (int): The number of missing points to generate.
        conversion_percentage (float): The percentage of black pixels to convert to white.
    """
    # Open the input image.
    image = Image.open(input_path)

    # Create a drawing tool for manipulating the image.
    draw = ImageDraw.Draw(image)

    # Determine the image's width and height in pixels.
    width = image.size[0]
    height = image.size[1]

    # Load pixel values for the image.
    pix = image.load()

    # Define a factor for introducing noise.
    noise_factor = 5

    # Loop through all pixels in the image.
    for i in range(width):
        for j in range(height):
            # Generate a random noise value within the specified factor.
            rand = random.randint(-noise_factor, noise_factor)

            # Add the noise to the Red, Green, and Blue (RGB) values of the pixel.
            red = pix[i, j][0] + rand
            green = pix[i, j][1] + rand
            blue = pix[i, j][2] + rand

            # Ensure that RGB values stay within the valid range (0-255).
            if red < 0:
                red = 0
            if green < 0:
                green = 0
            if blue < 0:
                blue = 0
            if red > 255:
                red = 255
            if green > 255:
                green = 255
            if blue > 255:
                blue = 255

            # Convert some black pixels to white based on the conversion percentage.
            if (red, green, blue) == (0, 0, 0) and random.random() < conversion_percentage:
                red, green, blue = 255, 255, 255

            # Set the pixel color accordingly.
            draw.point((i, j), (red, green, blue))

    # Generate missing points in the image.
    for _ in range(num_missing_points):
        x = random.randint(0, width - 1)
        y = random.randint(0, height - 1)
        draw.point((x, y), (255, 255, 255))  # Set the missing point to white

    # Save the noisy image as a file.
    image.save(output_path, "JPEG")

    # Show the noisy image
    plt.imshow(image)
    plt.axis('off')  # Hide axes
    plt.show()


    # Clean up the drawing tool.
    del draw

from PIL import Image, ImageDraw
import random

def generateNoisyImages():
    # List of image file paths
    image_paths = [
        "/content/1.jpg",
        "/content/2.jpg",
        "/content/3.jpg",
        "/content/4.jpg",
        "/content/5.jpg"
    ]

    for i, image_path in enumerate(image_paths, start=1):
        noisy_image_path = f"/content/noisy{i}.jpg"
        # Specify the number of missing points and conversion percentage here
        getNoisyBinaryImage(image_path, noisy_image_path, num_missing_points=1000, conversion_percentage=0.1)
        print(f"Noisy image for {image_path} generated and saved as {noisy_image_path}")

# Generate noisy images with missing points and black-to-white conversion
generateNoisyImages()

"""پرسش چهارم

Q1
"""

import numpy as np
import tensorflow as tf
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Layer
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanSquaredError

# Load the California Housing dataset
data = fetch_california_housing()
X, y = data.data, data.target

# Normalize the data
scaler = StandardScaler()
X = scaler.fit_transform(X)
y = np.expand_dims(y, axis=-1)  # Expand dimensions to match Keras expectations

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import numpy as np
import sklearn.cluster as cl
from sklearn import metrics as met
class RBF:
    def __init__ (self, name:str):
        self.name = name

    def fit_centers (self, X:np.ndarray):
        self.KMN = cl.KMeans(n_clusters=self.nH)
        self.KMN.fit(X)
        self.C = self.KMN.cluster_centers_

    def fit (self, X:np.ndarray, Y:np.ndarray, nH:int, nEpoch:int=100, lr:float=1e-2):
        self.nX = X.shape[1]
        self.nY = Y.shape[1]
        self.nH = nH
        self.nEpoch = nEpoch
        self.lr = lr
        self.fit_centers(X)
        self.fit_wb(X, Y)

    def fit_wb (self, X:np.ndarray, Y:np.ndarray):
        D = self.get_distances(X)
        O1 = self.bf(D)
        self.W = np.random.uniform(-1, +1, (self.nH, self.nY))
        self.B = np.random.uniform(-1, +1, (self.nY))
        self.history = {'loss':[], 'accuracy':[]}
        O2 = self.model(O1)
        E = round(met.mean_squared_error(Y, O2), 4)
        A = round(self.accuracy(Y, O2), 4)
        self.history['loss'].append(E)
        self.history['accuracy'].append(A)
        print(f'Epoch: {0} -- Loss: {E} -- Accuracy: {A}')
        for I in range(self.nEpoch):
            for x, y in zip(O1, Y):
                for i in range(self.nH):
                    for j in range(self.nY):
                        o = self.model(x)
                        e = y[j] - o[j]
                        d = (o[j] * (1 - o[j]))
                        self.W[i, j] += self.lr * x[i] * e * d
                for j in range(self.nY):
                    o = self.model(x)
                    e = y[j] - o[j]
                    d = (o[j] * (1 - o[j]))
                    self.B[j] += self.lr * e * d
            O2 = self.model(O1)
            E = round(met.mean_squared_error(Y, O2), 4)
            A = round(self.accuracy(Y, O2), 4)
            self.history['loss'].append(E)
            self.history['accuracy'].append(A)
            print(f'Epoch: {I+1} -- Loss: {E} -- Accuracy: {A}')

    def get_distances (self, X:np.ndarray):
        N = X.shape[0]
        D = np.zeros((N, self.nH))
        for i in range(N):
            for j in range(self.nH):
                D[i, j] = np.linalg.norm(X[i] - self.C[j], ord=2)
        return D

    def bf (self, D:np.ndarray, a:float=10):
        return np.exp(-a*np.power(D, 2))

    def model (self, X:np.ndarray):

        Z = np.dot(X, self.W) + self.B
        O = 1/(1 + np.exp(-Z))
        return O

    def accuracy (self, Y:np.ndarray, O:np.ndarray):
        N = Y.shape[0]
        a = 0
        for i in range(N):
            if np.argmax(Y[i, :]) == np.argmax(O[i, :]):
                a += 1
        return a/N

    def predict (self, X:np.ndarray):
        D = self.get_distances(X)
        O1 = self.bf(D)
        O2 = self.model(O1)
        return np.argmax(O2, axis=1).reshape((-1, 1))
#Model = RBF('My First RBF')

"""Q2"""

from keras.utils import to_categorical
from sklearn.preprocessing import OneHotEncoder
import sklearn.preprocessing as pp
import sklearn.cluster as cl
nD = 100 # Data Size
M = np.array([[0, 0], [1, 0.6], [0.6, 0.9]]) # Clusters Center
S = 0.2 # Distribution Variance
nC, nX = M.shape
X = np.zeros((nD, nX))
Y = np.zeros((nD, 1))
# Creating Dataset
for i in range(nD):
    c = np.random.randint(nC)
    X[i, :] = M[c] + S*np.random.randn(nX)
    Y[i, 0] = c
OHE = pp.OneHotEncoder()
OHY = OHE.fit_transform(Y).toarray()

Model = RBF('My First RBF')

Model.fit(X, OHY, 3, nEpoch=50, lr=1e-3)

"""Q6"""

Losses = Model.history['loss']
Accuracies = Model.history['accuracy']

plt.subplot(1, 2, 1)
plt.plot(Losses, lw=1.2, c='crimson', marker='o', ms=3)
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE')

plt.subplot(1, 2, 2)
plt.plot(Accuracies, lw=1.2, c='teal', marker='o', ms=3)
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

plt.show()

"""Q4-Q5"""

model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)),  # لایه پنهان
    tf.keras.layers.Dense(1)  # لایه خروجی
])

# کامپایل و آموزش مدل
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
              loss='mse',
              metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=1)

"""Q6"""

import matplotlib.pyplot as plt
Losses = model.history.history['loss']
Accuracies = model.history.history['accuracy']

plt.subplot(1, 2, 1)
plt.plot(Losses, lw=1.2, c='crimson', marker='o', ms=3)
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE')

plt.subplot(1, 2, 2)
plt.plot(Accuracies, lw=1.2, c='teal', marker='o', ms=3)
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

plt.show()
